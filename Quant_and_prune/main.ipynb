{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in /Users/kanishkaman/Desktop/memory-optimisation-of-llm/.venv/lib/python3.9/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/kanishkaman/Desktop/memory-optimisation-of-llm/.venv/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-19.0.1-cp39-cp39-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /Users/kanishkaman/Desktop/memory-optimisation-of-llm/.venv/lib/python3.9/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/kanishkaman/Desktop/memory-optimisation-of-llm/.venv/lib/python3.9/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/kanishkaman/Desktop/memory-optimisation-of-llm/.venv/lib/python3.9/site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py39-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Using cached aiohttp-3.11.14-cp39-cp39-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/kanishkaman/Desktop/memory-optimisation-of-llm/.venv/lib/python3.9/site-packages (from datasets) (0.29.3)\n",
      "Requirement already satisfied: packaging in /Users/kanishkaman/Desktop/memory-optimisation-of-llm/.venv/lib/python3.9/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/kanishkaman/Desktop/memory-optimisation-of-llm/.venv/lib/python3.9/site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp->datasets)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Using cached frozenlist-1.5.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Using cached multidict-6.2.0-cp39-cp39-macosx_11_0_arm64.whl.metadata (4.9 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.3.1-cp39-cp39-macosx_11_0_arm64.whl.metadata (10 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Using cached yarl-1.18.3-cp39-cp39-macosx_11_0_arm64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/kanishkaman/Desktop/memory-optimisation-of-llm/.venv/lib/python3.9/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/kanishkaman/Desktop/memory-optimisation-of-llm/.venv/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kanishkaman/Desktop/memory-optimisation-of-llm/.venv/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/kanishkaman/Desktop/memory-optimisation-of-llm/.venv/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kanishkaman/Desktop/memory-optimisation-of-llm/.venv/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/kanishkaman/Desktop/memory-optimisation-of-llm/.venv/lib/python3.9/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/kanishkaman/Desktop/memory-optimisation-of-llm/.venv/lib/python3.9/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/kanishkaman/Desktop/memory-optimisation-of-llm/.venv/lib/python3.9/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/kanishkaman/Desktop/memory-optimisation-of-llm/.venv/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Using cached aiohttp-3.11.14-cp39-cp39-macosx_11_0_arm64.whl (456 kB)\n",
      "Downloading multiprocess-0.70.16-py39-none-any.whl (133 kB)\n",
      "Using cached pyarrow-19.0.1-cp39-cp39-macosx_12_0_arm64.whl (30.7 MB)\n",
      "Downloading xxhash-3.5.0-cp39-cp39-macosx_11_0_arm64.whl (30 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached frozenlist-1.5.0-cp39-cp39-macosx_11_0_arm64.whl (52 kB)\n",
      "Using cached multidict-6.2.0-cp39-cp39-macosx_11_0_arm64.whl (29 kB)\n",
      "Downloading propcache-0.3.1-cp39-cp39-macosx_11_0_arm64.whl (46 kB)\n",
      "Using cached yarl-1.18.3-cp39-cp39-macosx_11_0_arm64.whl (92 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.3.0\n",
      "    Uninstalling fsspec-2025.3.0:\n",
      "      Successfully uninstalled fsspec-2025.3.0\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.14 aiosignal-1.3.2 async-timeout-5.0.1 attrs-25.3.0 datasets-3.4.1 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.12.0 multidict-6.2.0 multiprocess-0.70.16 propcache-0.3.1 pyarrow-19.0.1 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GPT2LMHeadModel\n",
    "import torch.nn.utils.prune as prune\n",
    "import time\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from torch.ao.quantization import quantize_dynamic\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration\n",
    "MODELS = {\n",
    "    \"distilgpt2\": \"distilgpt2\",\n",
    "    \"gpt2\": \"gpt2\",\n",
    "    # \"gpt2-medium\": \"gpt2-medium\"\n",
    "}\n",
    "# PRUNING_RATES = [0.0, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "PRUNING_RATES = [0.0, 0.4, 0.7]\n",
    "EVAL_SAMPLES = 100  # Number of samples to evaluate on\n",
    "WARMUP_RUNS = 5     # Number of warmup runs for inference timing\n",
    "MEASURE_RUNS = 10   # Number of runs to average for measurements\n",
    "MAX_SEQ_LENGTH = 128\n",
    "RESULTS_DIR = \"compression_results\"\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
    "\n",
    "# Helper functions\n",
    "def get_model_size_mb(model):\n",
    "    \"\"\"Calculate model size in MB (parameters only)\"\"\"\n",
    "    return sum(p.numel() * p.element_size() for p in model.parameters()) / (1024 * 1024)\n",
    "\n",
    "def get_model_total_size_mb(model):\n",
    "    \"\"\"Calculate total model size including buffers\"\"\"\n",
    "    total_size = 0\n",
    "    for param in model.parameters():\n",
    "        total_size += param.numel() * param.element_size()\n",
    "    for buffer in model.buffers():\n",
    "        total_size += buffer.numel() * buffer.element_size()\n",
    "    return total_size / (1024 * 1024)\n",
    "\n",
    "def count_non_zero_params(model):\n",
    "    \"\"\"Count non-zero parameters in the model\"\"\"\n",
    "    return sum(torch.count_nonzero(p) for p in model.parameters())\n",
    "\n",
    "def calculate_sparsity(model):\n",
    "    \"\"\"Calculate the actual sparsity percentage of the model\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    nonzero_params = count_non_zero_params(model)\n",
    "    return 100 * (1 - nonzero_params / total_params)\n",
    "\n",
    "def calculate_perplexity(model, eval_dataloader, device):\n",
    "    \"\"\"Calculate perplexity on evaluation dataset\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_length = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_dataloader, desc=\"Calculating perplexity\"):\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            # Calculate loss only on non-padded tokens\n",
    "            loss = outputs.loss\n",
    "            total_loss += loss.item() * inputs[\"input_ids\"].size(0)\n",
    "            total_length += inputs[\"input_ids\"].size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_length\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    return perplexity\n",
    "\n",
    "def measure_inference_time(model, inputs, n_runs=10):\n",
    "    \"\"\"Measure average inference time over multiple runs\"\"\"\n",
    "    model.eval()\n",
    "    # Warmup runs\n",
    "    with torch.no_grad():\n",
    "        for _ in range(WARMUP_RUNS):\n",
    "            _ = model(**inputs)\n",
    "\n",
    "    # Actual measurement\n",
    "    latencies = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_runs):\n",
    "            start_time = time.time()\n",
    "            _ = model(**inputs)\n",
    "            latencies.append(time.time() - start_time)\n",
    "\n",
    "    return {\n",
    "        \"mean\": np.mean(latencies),\n",
    "        \"std\": np.std(latencies),\n",
    "        \"min\": np.min(latencies),\n",
    "        \"max\": np.max(latencies)\n",
    "    }\n",
    "\n",
    "def prune_model(model, amount=0.5):\n",
    "    \"\"\"Apply L1 unstructured pruning to all linear layers\"\"\"\n",
    "    for name, module in tqdm(model.named_modules(), desc=f\"Pruning Model ({amount*100:.1f}%)\"):\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            prune.l1_unstructured(module, name='weight', amount=amount)\n",
    "\n",
    "    # Make pruning permanent to save memory\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            prune.remove(module, 'weight')\n",
    "\n",
    "    return model\n",
    "\n",
    "def prepare_eval_dataset(tokenizer, num_samples=100):\n",
    "    \"\"\"Prepare dataset for evaluation\"\"\"\n",
    "    # Load WikiText for perplexity evaluation\n",
    "    dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "\n",
    "    # Take a subset for faster evaluation\n",
    "    dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "\n",
    "    # Tokenize the dataset\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=\"max_length\",\n",
    "                         truncation=True, max_length=MAX_SEQ_LENGTH,\n",
    "                         return_tensors=\"pt\")\n",
    "\n",
    "    tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "    tokenized_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "\n",
    "    # Create labels for causal language modeling (shift input_ids)\n",
    "    def prepare_clm_inputs(examples):\n",
    "        examples[\"labels\"] = examples[\"input_ids\"].clone()\n",
    "        return examples\n",
    "\n",
    "    tokenized_dataset = tokenized_dataset.map(prepare_clm_inputs)\n",
    "    return tokenized_dataset\n",
    "\n",
    "def evaluate_model(model_name, pruning_rates, use_quantization=False):\n",
    "    \"\"\"Evaluate model with different pruning rates and optional quantization\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # Prepare evaluation dataset\n",
    "    eval_dataset = prepare_eval_dataset(tokenizer, EVAL_SAMPLES)\n",
    "    eval_dataloader = torch.utils.data.DataLoader(\n",
    "        eval_dataset, batch_size=4, shuffle=False\n",
    "    )\n",
    "\n",
    "    # Single input for latency testing\n",
    "    sample_text = \"The future of artificial intelligence is\"\n",
    "    sample_inputs = tokenizer(sample_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for pruning_rate in pruning_rates:\n",
    "        # Load a fresh model for each pruning rate\n",
    "        print(f\"\\n--- Evaluating {model_name} with pruning rate {pruning_rate:.2f} ---\")\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        model.to(device)\n",
    "\n",
    "        # Measure baseline\n",
    "        baseline_size = get_model_size_mb(model)\n",
    "        baseline_time = measure_inference_time(model, sample_inputs, MEASURE_RUNS)\n",
    "        baseline_perplexity = calculate_perplexity(model, eval_dataloader, device)\n",
    "        baseline_sparsity = calculate_sparsity(model)\n",
    "\n",
    "        # Apply pruning if rate > 0\n",
    "        if pruning_rate > 0:\n",
    "            model = prune_model(model, pruning_rate)\n",
    "\n",
    "        # Measure pruned model (before quantization)\n",
    "        pruned_size = get_model_size_mb(model)\n",
    "        pruned_time = measure_inference_time(model, sample_inputs, MEASURE_RUNS)\n",
    "        pruned_perplexity = calculate_perplexity(model, eval_dataloader, device)\n",
    "        pruned_sparsity = calculate_sparsity(model)\n",
    "\n",
    "        # Apply quantization if requested\n",
    "        if use_quantization:\n",
    "            print(\"Applying quantization...\")\n",
    "            quantized_model = quantize_dynamic(\n",
    "                model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "            )\n",
    "            quantized_model.to(device)\n",
    "\n",
    "            # Measure quantized model\n",
    "            quantized_size = get_model_size_mb(quantized_model)\n",
    "            quantized_time = measure_inference_time(quantized_model, sample_inputs, MEASURE_RUNS)\n",
    "            quantized_perplexity = calculate_perplexity(quantized_model, eval_dataloader, device)\n",
    "            quantized_sparsity = calculate_sparsity(quantized_model)\n",
    "\n",
    "            # Record results\n",
    "            results.append({\n",
    "                \"model\": model_name,\n",
    "                \"technique\": \"pruning+quantization\",\n",
    "                \"pruning_rate\": pruning_rate,\n",
    "                \"perplexity\": quantized_perplexity,\n",
    "                \"size_mb\": quantized_size,\n",
    "                \"latency_ms\": quantized_time[\"mean\"] * 1000,\n",
    "                \"latency_std_ms\": quantized_time[\"std\"] * 1000,\n",
    "                \"sparsity\": quantized_sparsity\n",
    "            })\n",
    "        else:\n",
    "            # Record results for pruning only\n",
    "            results.append({\n",
    "                \"model\": model_name,\n",
    "                \"technique\": \"pruning\",\n",
    "                \"pruning_rate\": pruning_rate,\n",
    "                \"perplexity\": pruned_perplexity,\n",
    "                \"size_mb\": pruned_size,\n",
    "                \"latency_ms\": pruned_time[\"mean\"] * 1000,\n",
    "                \"latency_std_ms\": pruned_time[\"std\"] * 1000,\n",
    "                \"sparsity\": pruned_sparsity\n",
    "            })\n",
    "\n",
    "        # Free memory\n",
    "        del model\n",
    "        if use_quantization:\n",
    "            del quantized_model\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def create_visualizations(results_df):\n",
    "    \"\"\"Create and save visualizations from results\"\"\"\n",
    "    sns.set(style=\"whitegrid\")\n",
    "\n",
    "    # Ensure pruning_rate is numeric\n",
    "    results_df[\"pruning_rate\"] = results_df[\"pruning_rate\"].astype(float)\n",
    "\n",
    "    # 1. Size Reduction vs Pruning Rate (by Model and Technique)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.lineplot(data=results_df, x=\"pruning_rate\", y=\"size_mb\",\n",
    "                 hue=\"model\", style=\"technique\", markers=True, dashes=False)\n",
    "    plt.title(\"Model Size vs Pruning Rate\", fontsize=16)\n",
    "    plt.xlabel(\"Pruning Rate\", fontsize=14)\n",
    "    plt.ylabel(\"Model Size (MB)\", fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.savefig(f\"{RESULTS_DIR}/size_vs_pruning.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # 2. Perplexity vs Pruning Rate (by Model and Technique)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.lineplot(data=results_df, x=\"pruning_rate\", y=\"perplexity\",\n",
    "                 hue=\"model\", style=\"technique\", markers=True, dashes=False)\n",
    "    plt.title(\"Perplexity vs Pruning Rate\", fontsize=16)\n",
    "    plt.xlabel(\"Pruning Rate\", fontsize=14)\n",
    "    plt.ylabel(\"Perplexity (lower is better)\", fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.savefig(f\"{RESULTS_DIR}/perplexity_vs_pruning.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # 3. Latency vs Pruning Rate (by Model and Technique)\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.lineplot(data=results_df, x=\"pruning_rate\", y=\"latency_ms\",\n",
    "                 hue=\"model\", style=\"technique\", markers=True, dashes=False)\n",
    "    plt.title(\"Inference Latency vs Pruning Rate\", fontsize=16)\n",
    "    plt.xlabel(\"Pruning Rate\", fontsize=14)\n",
    "    plt.ylabel(\"Latency (ms)\", fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.savefig(f\"{RESULTS_DIR}/latency_vs_pruning.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # 4. Compression Efficiency: Perplexity vs Size\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for model in results_df[\"model\"].unique():\n",
    "        model_data = results_df[results_df[\"model\"] == model]\n",
    "        techniques = model_data[\"technique\"].unique()\n",
    "\n",
    "        for technique in techniques:\n",
    "            data = model_data[model_data[\"technique\"] == technique]\n",
    "            plt.plot(data[\"size_mb\"], data[\"perplexity\"],\n",
    "                     marker='o', label=f\"{model} - {technique}\")\n",
    "\n",
    "            # Annotate points with pruning rate\n",
    "            for i, row in data.iterrows():\n",
    "                plt.annotate(f\"{row['pruning_rate']:.1f}\",\n",
    "                             (row[\"size_mb\"], row[\"perplexity\"]),\n",
    "                             textcoords=\"offset points\",\n",
    "                             xytext=(0,10),\n",
    "                             ha='center')\n",
    "\n",
    "    plt.title(\"Compression Efficiency: Perplexity vs Model Size\", fontsize=16)\n",
    "    plt.xlabel(\"Model Size (MB)\", fontsize=14)\n",
    "    plt.ylabel(\"Perplexity (lower is better)\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{RESULTS_DIR}/perplexity_vs_size.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # 5. Memory-Latency Tradeoff\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    for model in results_df[\"model\"].unique():\n",
    "        model_data = results_df[results_df[\"model\"] == model]\n",
    "        techniques = model_data[\"technique\"].unique()\n",
    "\n",
    "        for technique in techniques:\n",
    "            data = model_data[model_data[\"technique\"] == technique]\n",
    "            plt.plot(data[\"size_mb\"], data[\"latency_ms\"],\n",
    "                     marker='o', label=f\"{model} - {technique}\")\n",
    "\n",
    "            # Annotate points with pruning rate\n",
    "            for i, row in data.iterrows():\n",
    "                plt.annotate(f\"{row['pruning_rate']:.1f}\",\n",
    "                             (row[\"size_mb\"], row[\"latency_ms\"]),\n",
    "                             textcoords=\"offset points\",\n",
    "                             xytext=(0,10),\n",
    "                             ha='center')\n",
    "\n",
    "    plt.title(\"Memory-Latency Tradeoff\", fontsize=16)\n",
    "    plt.xlabel(\"Model Size (MB)\", fontsize=14)\n",
    "    plt.ylabel(\"Latency (ms)\", fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True)\n",
    "    plt.savefig(f\"{RESULTS_DIR}/latency_vs_size.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # 6. Sparsity Bar Chart - FIXED\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    # Create a categorical version of pruning_rate for the bar chart\n",
    "    results_df[\"pruning_rate_str\"] = results_df[\"pruning_rate\"].astype(str)\n",
    "\n",
    "    sns.barplot(data=results_df, x=\"pruning_rate_str\", y=\"sparsity\", hue=\"technique\",\n",
    "                errorbar=None, palette=\"viridis\")\n",
    "    plt.title(\"Achieved Sparsity by Model and Technique\", fontsize=16)\n",
    "    plt.xlabel(\"Target Pruning Rate\", fontsize=14)\n",
    "    plt.ylabel(\"Actual Sparsity (%)\", fontsize=14)\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.legend(title=\"Technique\", fontsize=12)\n",
    "    plt.savefig(f\"{RESULTS_DIR}/sparsity_by_technique.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # 7. Scatterplot matrix for all metrics\n",
    "    metrics = [\"pruning_rate\", \"size_mb\", \"latency_ms\", \"perplexity\", \"sparsity\"]\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    g = sns.pairplot(results_df, vars=metrics, hue=\"model\", palette=\"colorblind\",\n",
    "                      markers=[\"o\", \"s\", \"D\"], height=3, aspect=1.2)\n",
    "    g.fig.suptitle(\"Relationships Between Compression Metrics\", fontsize=20, y=1.02)\n",
    "    plt.savefig(f\"{RESULTS_DIR}/metrics_relationships.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    all_results = []\n",
    "\n",
    "    # Evaluate pruning only\n",
    "    for model_name in MODELS.values():\n",
    "        results_df = evaluate_model(model_name, PRUNING_RATES, use_quantization=False)\n",
    "        all_results.append(results_df)\n",
    "\n",
    "    # Evaluate pruning + quantization\n",
    "    for model_name in MODELS.values():\n",
    "        results_df = evaluate_model(model_name, PRUNING_RATES, use_quantization=True)\n",
    "        all_results.append(results_df)\n",
    "\n",
    "    # Combine all results\n",
    "    combined_results = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "    # Save results to CSV\n",
    "    combined_results.to_csv(f\"{RESULTS_DIR}/compression_results.csv\", index=False)\n",
    "\n",
    "    # Create visualizations\n",
    "    create_visualizations(combined_results)\n",
    "\n",
    "    # Print summary\n",
    "    print(\"\\n--- Summary of Best Configurations ---\")\n",
    "\n",
    "    # Best size reduction with acceptable perplexity\n",
    "    acceptable_perplexity = combined_results[combined_results[\"pruning_rate\"] == 0][\"perplexity\"].mean() * 1.5\n",
    "    acceptable_results = combined_results[combined_results[\"perplexity\"] <= acceptable_perplexity]\n",
    "\n",
    "    if not acceptable_results.empty:\n",
    "        best_compression = acceptable_results.loc[acceptable_results[\"size_mb\"].idxmin()]\n",
    "        print(f\"\\nBest Size Reduction (with perplexity < {acceptable_perplexity:.2f}):\")\n",
    "        print(f\"  Model: {best_compression['model']}\")\n",
    "        print(f\"  Technique: {best_compression['technique']}\")\n",
    "        print(f\"  Pruning Rate: {best_compression['pruning_rate']:.2f}\")\n",
    "        print(f\"  Size: {best_compression['size_mb']:.2f} MB\")\n",
    "        print(f\"  Perplexity: {best_compression['perplexity']:.2f}\")\n",
    "        print(f\"  Latency: {best_compression['latency_ms']:.2f} ms\")\n",
    "\n",
    "    # Best perplexity with significant compression\n",
    "    min_compression = 0.5  # At least 50% size reduction\n",
    "    baseline_sizes = {model: combined_results[(combined_results[\"model\"] == model) &\n",
    "                                             (combined_results[\"pruning_rate\"] == 0)][\"size_mb\"].values[0]\n",
    "                     for model in combined_results[\"model\"].unique()}\n",
    "\n",
    "    compressed_results = combined_results.copy()\n",
    "    for i, row in compressed_results.iterrows():\n",
    "        baseline = baseline_sizes[row[\"model\"]]\n",
    "        compressed_results.at[i, \"compression_ratio\"] = baseline / row[\"size_mb\"]\n",
    "\n",
    "    good_compression = compressed_results[compressed_results[\"compression_ratio\"] >= min_compression]\n",
    "\n",
    "    if not good_compression.empty:\n",
    "        best_quality = good_compression.loc[good_compression[\"perplexity\"].idxmin()]\n",
    "        print(f\"\\nBest Quality (with compression ratio >= {min_compression:.1f}):\")\n",
    "        print(f\"  Model: {best_quality['model']}\")\n",
    "        print(f\"  Technique: {best_quality['technique']}\")\n",
    "        print(f\"  Pruning Rate: {best_quality['pruning_rate']:.2f}\")\n",
    "        print(f\"  Size: {best_quality['size_mb']:.2f} MB\")\n",
    "        print(f\"  Compression Ratio: {best_quality['compression_ratio']:.2f}x\")\n",
    "        print(f\"  Perplexity: {best_quality['perplexity']:.2f}\")\n",
    "        print(f\"  Latency: {best_quality['latency_ms']:.2f} ms\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
