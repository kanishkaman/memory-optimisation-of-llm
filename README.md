# LLM Memory Optimization Techniques

This project explores and implements various techniques to optimize the memory usage and improve the efficiency of Large Language Models (LLMs) during inference and potentially training.

High memory requirements are a significant bottleneck for deploying large models. We are investigating methods to make these powerful models more accessible and faster on resource-constrained hardware, plus our goal is to combine some of these methods together, for further memory reduction.

**Techniques Explored:** **Pruning**, **Quantization**, **KV Caching**, **Offloading**, etc.
