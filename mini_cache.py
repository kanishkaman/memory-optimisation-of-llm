# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r73mCLduCsqW2vQ4F1ksIsNIBx36RWDA
"""

!pip install torch transformers

import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import time
import psutil
import os
import matplotlib.pyplot as plt
import numpy as np
from tqdm import tqdm

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"Using device: {device}")

model = GPT2LMHeadModel.from_pretrained('gpt2', output_attentions=True, use_cache=True).to(device)
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
model.eval()

def get_tensor_size_mb(tensor):
    """Calculate size of tensor in MB"""
    return tensor.element_size() * tensor.nelement() / (1024 * 1024)

def get_cache_size_mb(cache):
    """Calculate the total size of a KV cache in MB"""
    total_mb = 0
    for k, v in cache:
        total_mb += get_tensor_size_mb(k) + get_tensor_size_mb(v)
    return total_mb

def slerp(a, b, t=0.5):
    """Simple SLERP-like interpolation (assuming vectors are normalized)"""
    return (1 - t) * a + t * b

def run_with_minicache(prompt, merge_layers=[(4, 5), (8, 9)], retain_ratio=0.05, max_new_tokens=20):
    """
    MiniCache implementation with improved memory tracking
    - Multiple layer pairs can be merged
    - Each layer pair gets compressed with the specified retention ratio
    """
    inputs = tokenizer(prompt, return_tensors='pt').to(device)

    # First run the prefill phase to get the cache
    with torch.no_grad():
        outputs = model(**inputs, output_hidden_states=True, output_attentions=True)
        cache = outputs.past_key_values
        hidden_states = outputs.hidden_states
        attention = outputs.attentions

    # Keep a copy of the original cache to measure size
    orig_cache = [(k.clone(), v.clone()) for k, v in cache]
    original_cache_size_mb = get_cache_size_mb(orig_cache)

    # Apply compression to each specified layer pair
    modified_cache = list(orig_cache)
    for l1, l2 in merge_layers:
        # Cross-layer compression
        k1, v1 = orig_cache[l1]
        k2, v2 = orig_cache[l2]

        # Merge with SLERP-style linear interpolation
        k_merge = slerp(k1, k2, t=0.5)
        v_merge = slerp(v1, v2, t=0.5)

        # Retain top tokens based on attention scores averaged over heads
        attn_scores = attention[l2].mean(dim=1)  # (batch, query_len, key_len)
        token_scores = attn_scores[:, -1, :]      # focus on last query token's scores
        topk = max(1, int(token_scores.size(-1) * retain_ratio))  # Ensure at least one token
        _, topk_idx = torch.topk(token_scores, topk, dim=-1)

        # Select retained keys/values from k2, v2
        retained_k = torch.stack([k2[b, :, topk_idx[b], :] for b in range(k2.size(0))], dim=0)
        retained_v = torch.stack([v2[b, :, topk_idx[b], :] for b in range(v2.size(0))], dim=0)

        # Update the modified cache
        modified_cache[l2] = (
            torch.cat([k_merge, retained_k], dim=2),
            torch.cat([v_merge, retained_v], dim=2),
        )

        # Empty l1's cache (nearly empty, keep minimal representation)
        empty_k = k1[:, :, :1, :]  # Keep same batch/head/head_dim, minimal seq_len
        empty_v = v1[:, :, :1, :]
        modified_cache[l1] = (empty_k, empty_v)

    # Calculate compressed cache size
    compressed_cache_size_mb = get_cache_size_mb(modified_cache)
    compression_ratio = original_cache_size_mb / compressed_cache_size_mb if compressed_cache_size_mb > 0 else 0

    # Calculate storage savings
    storage_saved_mb = original_cache_size_mb - compressed_cache_size_mb

    # Manual decoding with the modified cache
    start_time = time.time()

    input_ids = inputs['input_ids']
    generated_ids = input_ids.clone()
    curr_cache = modified_cache

    for _ in range(max_new_tokens):
        with torch.no_grad():
            outputs = model(
                generated_ids[:, -1:],
                past_key_values=curr_cache,
                use_cache=True
            )

            # Update cache for next iteration
            curr_cache = outputs.past_key_values

            # Get the predicted token
            next_token_logits = outputs.logits[:, -1, :]
            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)

            # Append to the sequence
            generated_ids = torch.cat([generated_ids, next_token], dim=1)

    end_time = time.time()

    # Decode the full sequence
    decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

    return {
        "output": decoded,
        "inference_time": end_time - start_time,
        "tokens_per_second": max_new_tokens / (end_time - start_time),
        "original_cache_mb": original_cache_size_mb,
        "compressed_cache_mb": compressed_cache_size_mb,
        "compression_ratio": compression_ratio,
        "storage_saved_mb": storage_saved_mb
    }

def run_baseline(prompt, max_new_tokens=20):
    inputs = tokenizer(prompt, return_tensors='pt').to(device)

    # First run the prefill phase to get the cache
    with torch.no_grad():
        outputs = model(**inputs)
        cache = outputs.past_key_values

    # Measure original cache size
    original_cache_size_mb = get_cache_size_mb(cache)

    # Manual decoding
    start_time = time.time()

    input_ids = inputs['input_ids']
    generated_ids = input_ids.clone()
    curr_cache = cache

    for _ in range(max_new_tokens):
        with torch.no_grad():
            outputs = model(
                generated_ids[:, -1:],
                past_key_values=curr_cache,
                use_cache=True
            )

            # Update cache for next iteration
            curr_cache = outputs.past_key_values

            # Get the predicted token
            next_token_logits = outputs.logits[:, -1, :]
            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)

            # Append to the sequence
            generated_ids = torch.cat([generated_ids, next_token], dim=1)

    end_time = time.time()

    # Decode the full sequence
    decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)

    return {
        "output": decoded,
        "inference_time": end_time - start_time,
        "tokens_per_second": max_new_tokens / (end_time - start_time),
        "original_cache_mb": original_cache_size_mb,
        "compressed_cache_mb": original_cache_size_mb,  # No compression
        "compression_ratio": 1.0,
        "storage_saved_mb": 0.0
    }

def benchmark(prompts, retain_ratios=None, max_new_tokens=20, layer_pairs=None):
    if retain_ratios is None:
        retain_ratios = [0.01, 0.05, 0.1, 0.2]

    if layer_pairs is None:
        layer_pairs = [
            [(4, 5), (8, 9)],        # 2 layer pairs
            [(2, 3), (6, 7), (10, 11)],    # 3 layer pairs
            [(i, i+1) for i in range(0, 11, 2)]  # 6 layer pairs
        ]

    results = {
        "baseline": [],
        "minicache": {f"{len(pairs)}_pairs_{ratio}": [] for pairs in layer_pairs for ratio in retain_ratios}
    }

    for prompt in tqdm(prompts, desc="Benchmarking prompts"):
        # Run baseline
        baseline_result = run_baseline(prompt, max_new_tokens)
        results["baseline"].append(baseline_result)

        # Run with different configurations
        for pairs in layer_pairs:
            for ratio in retain_ratios:
                key = f"{len(pairs)}_pairs_{ratio}"
                minicache_result = run_with_minicache(
                    prompt,
                    merge_layers=pairs,
                    retain_ratio=ratio,
                    max_new_tokens=max_new_tokens
                )
                results["minicache"][key].append(minicache_result)

    return results

def plot_results(results):
    # Extract configuration details
    configs = list(results["minicache"].keys())

    # Organize data by number of layer pairs
    pairs_data = {}
    for config in configs:
        parts = config.split('_')
        if len(parts) == 3 :
          pairs, _, ratio = parts
        else:
          continue
        num_pairs = int(pairs)
        ratio = float(ratio)

        if num_pairs not in pairs_data:
            pairs_data[num_pairs] = {"ratios": [], "compression": [], "speedup": [], "saved_mb": []}

        pairs_data[num_pairs]["ratios"].append(ratio)

        # Get averages
        baseline_time = np.mean([r["inference_time"] for r in results["baseline"]])
        minicache_time = np.mean([r["inference_time"] for r in results["minicache"][config]])
        speedup = baseline_time / minicache_time if minicache_time > 0 else 0

        compression = np.mean([r["compression_ratio"] for r in results["minicache"][config]])
        saved_mb = np.mean([r["storage_saved_mb"] for r in results["minicache"][config]])

        pairs_data[num_pairs]["compression"].append(compression)
        pairs_data[num_pairs]["speedup"].append(speedup)
        pairs_data[num_pairs]["saved_mb"].append(saved_mb)

    # Plot results
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    # Colors for different numbers of layer pairs
    colors = ['crimson', 'royalblue', 'forestgreen', 'darkorange']

    # Plot 1: Compression ratio vs Retain ratio
    ax = axes[0, 0]
    for i, (num_pairs, data) in enumerate(sorted(pairs_data.items())):
        ax.plot(data["ratios"], data["compression"], 'o-',
                label=f"{num_pairs} layer pairs",
                color=colors[i % len(colors)])

    ax.set_xlabel('Retain Ratio')
    ax.set_ylabel('Compression Ratio')
    ax.set_title('KV Cache Compression Ratio')
    ax.grid(True, alpha=0.3)
    ax.legend()

    # Plot 2: Speedup vs Retain ratio
    ax = axes[0, 1]
    for i, (num_pairs, data) in enumerate(sorted(pairs_data.items())):
        ax.plot(data["ratios"], data["speedup"], 'o-',
                label=f"{num_pairs} layer pairs",
                color=colors[i % len(colors)])

    ax.set_xlabel('Retain Ratio')
    ax.set_ylabel('Speedup Factor')
    ax.set_title('Inference Speedup Factor')
    ax.grid(True, alpha=0.3)
    ax.legend()

    # Plot 3: Memory saved vs Retain ratio
    ax = axes[1, 0]
    for i, (num_pairs, data) in enumerate(sorted(pairs_data.items())):
        ax.plot(data["ratios"], data["saved_mb"], 'o-',
                label=f"{num_pairs} layer pairs",
                color=colors[i % len(colors)])

    ax.set_xlabel('Retain Ratio')
    ax.set_ylabel('Memory Saved (MB)')
    ax.set_title('KV Cache Memory Savings')
    ax.grid(True, alpha=0.3)
    ax.legend()

    # Plot 4: Memory saved vs number of layer pairs (at best retain ratio)
    ax = axes[1, 1]
    best_ratio = 0.01  # Usually the smallest ratio gives best compression

    # Get the best compression for each number of pairs
    num_pairs_list = sorted(pairs_data.keys())
    best_compression = []
    best_saved_mb = []

    for num_pairs in num_pairs_list:
        data = pairs_data[num_pairs]
        idx = data["ratios"].index(min(data["ratios"]))
        best_compression.append(data["compression"][idx])
        best_saved_mb.append(data["saved_mb"][idx])

    ax.bar(num_pairs_list, best_compression, color='mediumorchid', alpha=0.7, label='Compression Ratio')
    ax.set_xlabel('Number of Layer Pairs')
    ax.set_ylabel('Compression Ratio')
    ax.set_title(f'Best Compression (retain ratio = {best_ratio})')
    ax.grid(True, alpha=0.3, axis='y')

    ax2 = ax.twinx()
    ax2.plot(num_pairs_list, best_saved_mb, 'o-', color='darkorange', label='Memory Saved (MB)')
    ax2.set_ylabel('Memory Saved (MB)')

    lines, labels = ax.get_legend_handles_labels()
    lines2, labels2 = ax2.get_legend_handles_labels()
    ax.legend(lines + lines2, labels + labels2, loc='upper left')

    plt.tight_layout()
    plt.savefig('minicache_performance_analysis.png')

    return 'minicache_performance_analysis.png'

# Test prompts of different lengths
prompts = [
    "In a distant future, AI governs the universe.",
    "The quick brown fox jumps over the lazy dog. It was a warm summer day and all the animals were enjoying the sunshine.",
    "Quantum computing represents a paradigm shift in computational power, leveraging quantum mechanical phenomena such as superposition and entanglement to perform operations on data.",
    "Machine learning models have revolutionized many industries through their ability to find patterns in data that would be impossible for humans to detect manually. Deep learning, a subset of machine learning, uses neural networks with many layers to process complex information."
]

# Define layer pairs to test
layer_pairs = [
    [(4, 5), (8, 9)],          # 2 layer pairs
    [(2, 3), (6, 7), (10, 11)],      # 3 layer pairs
    [(i, i+1) for i in range(0, 11, 2)]  # 6 layer pairs
]

# Run benchmarks
print("Running benchmarks with multiple layer pair configurations...")
retain_ratios = [0.01, 0.05, 0.1, 0.2]
results = benchmark(prompts, retain_ratios, layer_pairs=layer_pairs)

# Print summary statistics
print("\nSUMMARY STATISTICS:")
print("===================\n")

# Baseline stats
baseline_time = np.mean([r["inference_time"] for r in results["baseline"]])
baseline_cache = np.mean([r["original_cache_mb"] for r in results["baseline"]])

print(f"BASELINE:")
print(f"  Average Inference Time: {baseline_time:.4f} seconds")
print(f"  Average KV Cache Size: {baseline_cache:.4f} MB\n")

# MiniCache stats for each configuration
for config in sorted(results["minicache"].keys()):
    parts = config.split('_')
    if len(parts) == 3:
        pairs, _, ratio_str = parts
        num_pairs = int(pairs)
        ratio = float(ratio_str)
        minicache_time = np.mean([r["inference_time"] for r in results["minicache"][config]])
        minicache_cache = np.mean([r["compressed_cache_mb"] for r in results["minicache"][config]])
        minicache_ratio = np.mean([r["compression_ratio"] for r in results["minicache"][config]])
        saved_mb = np.mean([r["storage_saved_mb"] for r in results["minicache"][config]])
        speedup = baseline_time / minicache_time if minicache_time > 0 else 0
        print(f"MINICACHE ({num_pairs} layer pairs, Retain Ratio = {ratio}):")
        print(f"  Average Inference Time: {minicache_time:.4f} seconds (Speedup: {speedup:.2f}x)")
        print(f"  Average KV Cache Size: {minicache_cache:.4f} MB")
        print(f"  Memory Saved: {saved_mb:.4f} MB")
        print(f"  Average Compression Ratio: {minicache_ratio:.2f}x\n")
    else:
        print(f"Unexpected config format: {config}")

# Generate plots
print("Generating performance analysis plot...")
performance_plot = plot_results(results)
print(f"Plot saved as {performance_plot}")

# Example of a single detailed run
print("\nDETAILED EXAMPLE RUN:")
print("====================\n")
prompt = "Quantum computing represents a paradigm shift in computational power, leveraging quantum mechanical phenomena such as superposition and entanglement to perform operations on data."

baseline_result = run_baseline(prompt)
minicache_result = run_with_minicache(prompt,
                                        merge_layers=[(i, i+1) for i in range(0, 11, 2)],  # 6 layer pairs
                                        retain_ratio=0.01)  # Aggressive compression

print(f"BASELINE:")
print(f"  KV Cache Size: {baseline_result['original_cache_mb']:.4f} MB")
print(f"  Inference Time: {baseline_result['inference_time']:.4f} seconds")
print(f"  Output: {baseline_result['output'][:100]}...\n")

print(f"MINICACHE (6 layer pairs, retain ratio=0.01):")
print(f"  Original KV Cache Size: {minicache_result['original_cache_mb']:.4f} MB")
print(f"  Compressed KV Cache Size: {minicache_result['compressed_cache_mb']:.4f} MB")
print(f"  Memory Saved: {minicache_result['storage_saved_mb']:.4f} MB")
print(f"  Compression Ratio: {minicache_result['compression_ratio']:.2f}x")
print(f"  Inference Time: {minicache_result['inference_time']:.4f} seconds")
print(f"  Output: {minicache_result['output'][:100]}...")