# -*- coding: utf-8 -*-
"""Untitled35.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w17SHhu6jUzfaw6tEIYSNUO_0ySjZ2_i
"""

import os
import time
import torch
import torch.nn as nn
import logging
import psutil
import gc
from dataclasses import dataclass
from typing import Dict, List, Tuple, Union, Optional, Set
import weakref

# Set up logging
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(message)s",
    level=logging.INFO,
    force=True
)
logger = logging.getLogger(__name__)

@dataclass
class ZIOConfig:
    """Configuration for Zero-Infinity Offloading."""
    cpu_offload: bool = True
    activation_checkpointing: bool = True
    optimizer_state_offloading: bool = True
    quantize_offloaded_parameters: bool = True
    quantize_offloaded_optimizer_states: bool = True
    checkpoint_activation_fraction: float = 0.5  # Increased for better memory savings
    offload_param_fraction: float = 0.7  # Fraction of parameters to offload
    prefetch_horizon: int = 2
    communication_dtype: torch.dtype = torch.bfloat16
    cpu_offload_quantization_bits: int = 8
    force_parameter_refresh: bool = False  # Force refresh parameters after backward

class ZeroInfinityOptimizer(torch.optim.Optimizer):
    """Improved wrapper around optimizer to implement Zero-Infinity Offloading."""

    def __init__(
        self,
        optimizer: torch.optim.Optimizer,
        model: nn.Module,
        config: ZIOConfig,
    ):
        self.optimizer = optimizer
        self.model = model
        self.config = config
        self.param_groups = optimizer.param_groups
        self.state = optimizer.state

        # Maps parameter to its offloaded CPU tensor
        self.offloaded_params = {}

        # Maps parameter to its offloaded optimizer state
        self.offloaded_states = {}

        # Track which parameters are currently on GPU
        self.active_params: Set[torch.nn.Parameter] = set()

        # Keep track of parameter sizes for efficient memory management
        self.param_sizes = {p: p.numel() * p.element_size() for group in self.param_groups
                           for p in group['params'] if p.requires_grad}

        # Determine parameters to offload based on configured fraction
        if self.config.offload_param_fraction > 0:
            self._setup_offload_parameters()

        # Initialize offloading
        if self.config.cpu_offload:
            self._initialize_offload()

    def _setup_offload_parameters(self):
        """Determine which parameters to offload based on size."""
        # Sort parameters by size (largest first for better memory savings)
        sorted_params = sorted(
            [(p, self.param_sizes[p]) for group in self.param_groups for p in group['params'] if p.requires_grad],
            key=lambda x: x[1],
            reverse=True
        )

        # Calculate total size and target offload size
        total_size = sum(size for _, size in sorted_params)
        target_offload_size = total_size * self.config.offload_param_fraction

        # Mark parameters for offloading until we reach the target size
        current_offload_size = 0
        self.params_to_offload = set()

        for param, size in sorted_params:
            if current_offload_size < target_offload_size:
                self.params_to_offload.add(param)
                current_offload_size += size
            else:
                break

        total_offload_mb = current_offload_size / (1024 * 1024)
        total_params_mb = total_size / (1024 * 1024)
        logger.info(f"Selected {len(self.params_to_offload)} parameters for offloading "
                   f"({total_offload_mb:.2f}MB / {total_params_mb:.2f}MB, "
                   f"{len(self.params_to_offload) / len(sorted_params) * 100:.1f}%)")

    def _initialize_offload(self):
        """Initialize CPU offloading for parameters and optimizer states."""
        if not self.config.cpu_offload:
            return

        # Offload selected parameters to CPU
        for group in self.param_groups:
            for p in group['params']:
                if p.requires_grad and p in self.params_to_offload:
                    self._offload_param_to_cpu(p)

    def _fetch_param_to_gpu(self, param):
        """Fetch parameter from CPU to GPU if offloaded."""
        if param not in self.offloaded_params:
            return

        with torch.no_grad():
            if self.config.quantize_offloaded_parameters:
                cpu_p, scale = self.offloaded_params[param]
                # Restore the parameter with original size
                original_shape = param.shape
                param.data = cpu_p.to(param.device).to(torch.float32) * scale.to(param.device)
                # Ensure the shape is correct
                if param.data.shape != original_shape:
                    param.data = param.data.reshape(original_shape)
            else:
                param.data = self.offloaded_params[param].to(param.device)

            # Track that this parameter is now on GPU
            self.active_params.add(param)

            # Remove from offloaded params to prevent duplicate storage
            del self.offloaded_params[param]

    def _offload_param_to_cpu(self, param):
        """Offload parameter from GPU to CPU."""
        # Skip if already offloaded
        if param in self.offloaded_params:
            return

        with torch.no_grad():
            if self.config.quantize_offloaded_parameters:
                # Handle zero tensor case
                if param.abs().max() == 0:
                    scale = torch.tensor(1.0, device=param.device)
                else:
                    scale = param.abs().max() / 127.0

                cpu_p = (param / scale).to(torch.int8).cpu()
                self.offloaded_params[param] = (cpu_p, scale.cpu())
            else:
                self.offloaded_params[param] = param.cpu()

            # Free GPU memory with placeholder - use a small tensor to retain shape info
            param.data = torch.zeros((1,), device=param.device)

            # Remove from active params tracking
            self.active_params.discard(param)

    def _fetch_state_to_gpu(self, param):
        """Fetch optimizer state from CPU to GPU if offloaded."""
        if param not in self.optimizer.state or param not in self.offloaded_states:
            return

        state = self.optimizer.state[param]

        for k, v in state.items():
            if isinstance(v, torch.Tensor) and k in self.offloaded_states.get(param, {}):
                if self.config.quantize_offloaded_optimizer_states:
                    cpu_v, scale = self.offloaded_states[param][k]
                    state[k] = cpu_v.to(param.device).to(torch.float32) * scale.to(param.device)
                else:
                    state[k] = self.offloaded_states[param][k].to(param.device)

                # Remove from offloaded states
                del self.offloaded_states[param][k]

    def _offload_state_to_cpu(self, param):
        """Offload optimizer state from GPU to CPU."""
        if param not in self.optimizer.state or not self.config.optimizer_state_offloading:
            return

        state = self.optimizer.state[param]
        if param not in self.offloaded_states:
            self.offloaded_states[param] = {}

        for k, v in list(state.items()):
            if isinstance(v, torch.Tensor):
                if self.config.quantize_offloaded_optimizer_states:
                    if v.abs().max() == 0:
                        scale = torch.tensor(1.0, device=v.device)
                    else:
                        scale = v.abs().max() / 127.0
                    cpu_v = (v / scale).to(torch.int8).cpu()
                    self.offloaded_states[param][k] = (cpu_v, scale.cpu())
                else:
                    self.offloaded_states[param][k] = v.cpu()

                # Replace with placeholder
                state[k] = torch.zeros((1,), device=v.device)

    def state_dict(self):
        """Return the state dict of the wrapped optimizer."""
        # Move all parameters/states back to GPU temporarily
        for group in self.param_groups:
            for p in group['params']:
                if p.requires_grad:
                    self._fetch_param_to_gpu(p)
                    self._fetch_state_to_gpu(p)

        # Get state dict from underlying optimizer
        state_dict = self.optimizer.state_dict()

        # Re-offload parameters and states
        for group in self.param_groups:
            for p in group['params']:
                if p.requires_grad and p in self.params_to_offload:
                    self._offload_param_to_cpu(p)
                    self._offload_state_to_cpu(p)

        return state_dict

    def load_state_dict(self, state_dict):
        """Load the state dict to the wrapped optimizer."""
        # Fetch all parameters to GPU
        for group in self.param_groups:
            for p in group['params']:
                if p.requires_grad:
                    self._fetch_param_to_gpu(p)

        # Load state dict
        result = self.optimizer.load_state_dict(state_dict)

        # Re-offload parameters and states
        for group in self.param_groups:
            for p in group['params']:
                if p.requires_grad and p in self.params_to_offload:
                    self._offload_param_to_cpu(p)
                    self._offload_state_to_cpu(p)

        return result

    def zero_grad(self, set_to_none=False):
        """Zero gradients for all parameters."""
        return self.optimizer.zero_grad(set_to_none=set_to_none)

    def step(self, closure=None):
        """Perform optimization step with parameter and optimizer state offloading."""
        # For each parameter that needs update
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None or not p.requires_grad:
                    continue

                # Fetch param and state to GPU for this update
                self._fetch_param_to_gpu(p)
                self._fetch_state_to_gpu(p)

        # Perform optimization step with the normal optimizer
        loss = self.optimizer.step(closure)

        # Offload updated parameters and states back to CPU
        for group in self.param_groups:
            for p in group['params']:
                if p.grad is None or not p.requires_grad:
                    continue

                if p in self.params_to_offload:
                    self._offload_param_to_cpu(p)
                    self._offload_state_to_cpu(p)

        # Force refresh parameters if configured
        if self.config.force_parameter_refresh:
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        return loss


class ActivationCheckpointer(torch.autograd.Function):
    """Custom autograd function for activation checkpointing."""

    @staticmethod
    def forward(ctx, run_function, preserve_rng_state, *args):
        ctx.run_function = run_function
        ctx.preserve_rng_state = preserve_rng_state

        # Save RNG state
        if preserve_rng_state:
            ctx.fwd_cpu_state = torch.get_rng_state()
            if torch.cuda.is_available():
                ctx.fwd_gpu_state = torch.cuda.get_rng_state()

        # Save non-tensor inputs
        ctx.non_tensor_inputs = []
        for i, arg in enumerate(args):
            if not isinstance(arg, torch.Tensor):
                ctx.non_tensor_inputs.append((i, arg))

        # Save only tensor inputs that require gradients
        ctx.save_for_backward(*[arg for arg in args if isinstance(arg, torch.Tensor) and arg.requires_grad])

        # Run forward with no gradient tracking
        with torch.no_grad():
            output = run_function(*args)

        return output

    @staticmethod
    def backward(ctx, *grad_outputs):
        # Restore RNG state
        if ctx.preserve_rng_state:
            cpu_state = ctx.fwd_cpu_state
            torch.set_rng_state(cpu_state)
            if torch.cuda.is_available() and hasattr(ctx, 'fwd_gpu_state'):
                torch.cuda.set_rng_state(ctx.fwd_gpu_state)

        # Recreate the inputs for the backward pass
        saved_tensors = ctx.saved_tensors
        tensor_inputs = saved_tensors

        # Run forward with gradient tracking to get outputs for backward pass
        with torch.enable_grad():
            outputs = ctx.run_function(*tensor_inputs)

        # Run backward pass
        torch.autograd.backward(outputs, grad_outputs)

        # Return gradients for each input
        grads = tuple(tensor.grad if tensor.requires_grad else None for tensor in tensor_inputs)

        # Add None for the two function arguments and non-tensor inputs
        all_grads = [None, None]  # For run_function and preserve_rng_state
        all_grads.extend(grads)

        return tuple(all_grads)


class ZeroInfinityOffloadModule(nn.Module):
    """Improved wrapper module that implements Zero-Infinity offloading techniques."""

    def __init__(
        self,
        model: nn.Module,
        config: ZIOConfig,
    ):
        super().__init__()
        self.model = model
        self.config = config

        # Apply activation checkpointing to eligible modules
        if config.activation_checkpointing:
            self._apply_activation_checkpointing()

    def _apply_activation_checkpointing(self):
        """Apply activation checkpointing to eligible layers."""
        # Find transformer layers to apply checkpointing
        transformer_layers = []

        # Identify transformer blocks in common model architectures
        for name, module in self.model.named_modules():
            # Look for decoder/transformer blocks in various architectures
            if any(layer_type in name.lower() for layer_type in ["block", "layer", "decoder", "transformer"]):
                if len(list(module.children())) > 2:  # Usually blocks have multiple children
                    transformer_layers.append((name, module))

        # Determine which layers to checkpoint based on configured fraction
        if transformer_layers:
            num_layers = len(transformer_layers)
            num_to_checkpoint = max(1, int(self.config.checkpoint_activation_fraction * num_layers))

            # Checkpoint every N layers to match the desired fraction
            checkpoint_interval = max(1, num_layers // num_to_checkpoint)

            logger.info(f"Found {num_layers} transformer layers, checkpointing every {checkpoint_interval}th layer")

            # Apply checkpointing to selected layers
            checkpointed_layers = 0
            for i in range(0, num_layers, checkpoint_interval):
                if i < len(transformer_layers):
                    name, layer = transformer_layers[i]
                    original_forward = layer.forward

                    # Replace forward method with checkpointed version
                    def make_checkpointed_forward(fwd_func):
                        def checkpointed_forward(*args, **kwargs):
                            # Convert kwargs to positional args for checkpointing
                            all_args = list(args)
                            for k, v in kwargs.items():
                                all_args.append(v)

                            def run_function(*inputs):
                                # Use the first len(args) inputs as positional args
                                # and the rest as keyword args
                                pos_args = inputs[:len(args)]
                                kw_args = {}
                                if kwargs:
                                    kw_values = inputs[len(args):]
                                    for (k, _), v in zip(kwargs.items(), kw_values):
                                        kw_args[k] = v
                                return fwd_func(*pos_args, **kw_args)

                            return ActivationCheckpointer.apply(run_function, True, *all_args)

                        return checkpointed_forward

                    # Apply the checkpointed forward
                    layer.forward = make_checkpointed_forward(original_forward)
                    checkpointed_layers += 1

                    logger.info(f"Applied checkpointing to layer {name}")

            logger.info(f"Applied activation checkpointing to {checkpointed_layers} layers")

    def forward(self, *args, **kwargs):
        """Forward pass with memory-efficient execution."""
        return self.model(*args, **kwargs)

def measure_memory_usage():
    """Measure GPU and CPU memory usage."""
    try:
        # GPU memory
        gpu_mem_alloc = torch.cuda.memory_allocated() / (1024 ** 3)
        gpu_mem_reserved = torch.cuda.memory_reserved() / (1024 ** 3)
    except:
        gpu_mem_alloc = 0
        gpu_mem_reserved = 0
        logger.warning("Failed to measure GPU memory, setting to 0")

    try:
        # CPU memory
        cpu_percent = psutil.virtual_memory().percent
        cpu_used_gb = psutil.virtual_memory().used / (1024 ** 3)
    except:
        cpu_percent = 0
        cpu_used_gb = 0
        logger.warning("Failed to measure CPU memory, setting to 0")

    return {
        "gpu_allocated_gb": gpu_mem_alloc,
        "gpu_reserved_gb": gpu_mem_reserved,
        "cpu_percent": cpu_percent,
        "cpu_used_gb": cpu_used_gb
    }


def print_memory_stats(stage: str, memory_usage: Dict[str, float]):
    """Print memory statistics in a formatted way."""
    logger.info(f"Memory usage at {stage}:")
    logger.info(f"  GPU Memory: {memory_usage['gpu_allocated_gb']:.2f} GB allocated, "
                f"{memory_usage['gpu_reserved_gb']:.2f} GB reserved")
    logger.info(f"  CPU Memory: {memory_usage['cpu_used_gb']:.2f} GB used "
                f"({memory_usage['cpu_percent']:.1f}%)")


def run_zio_training(
    model_name: str = "gpt2",
    batch_size: int = 4,
    seq_length: int = 512,
    apply_zio: bool = True,
    gpu_id: int = 0,
    vocab_size: int = 50257,  # GPT-2 vocab size
    learning_rate: float = 5e-5,
    num_iterations: int = 3,
):
    """Run training with or without Zero-Infinity Offloading and measure memory usage."""
    # Set device
    if torch.cuda.is_available():
        device = torch.device(f"cuda:{gpu_id}")
        logger.info(f"Using GPU: {torch.cuda.get_device_name(gpu_id)}")
    else:
        device = torch.device("cpu")
        logger.info("CUDA not available, using CPU")

    # Create memory tracking list
    memory_tracking = []

    # Clear cache
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()

    # Record baseline memory
    memory_tracking.append(("baseline", measure_memory_usage()))
    print_memory_stats("baseline", memory_tracking[0][1])

    # Load the model
    try:
        from transformers import GPT2LMHeadModel, GPT2Config, GPT2Tokenizer
        logger.info(f"Loading model: {model_name}")

        model = GPT2LMHeadModel.from_pretrained(model_name)
        logger.info(f"Model loaded successfully: {model_name}")
    except Exception as e:
        logger.error(f"Error loading model {model_name}: {e}")
        return None

    # Record memory after model loading
    model = model.to(device)
    memory_tracking.append(("model_loaded", measure_memory_usage()))
    print_memory_stats("model loaded", memory_tracking[-1][1])

    # Configure ZIO
    zio_config = ZIOConfig(
        cpu_offload=apply_zio,
        activation_checkpointing=apply_zio,
        optimizer_state_offloading=apply_zio,
        quantize_offloaded_parameters=apply_zio,
        quantize_offloaded_optimizer_states=apply_zio,
        checkpoint_activation_fraction=0.5,  # Checkpoint more layers for better savings
        offload_param_fraction=0.7,  # Offload 70% of parameters to CPU
        prefetch_horizon=2,
        force_parameter_refresh=apply_zio,  # Force refresh after backward to free memory
    )

    # Apply ZIO techniques if enabled
    if apply_zio:
        try:
            model = ZeroInfinityOffloadModule(model, zio_config)
            logger.info("Applied Zero-Infinity Offloading wrapper to model")
        except Exception as e:
            logger.error(f"Error applying ZIO to model: {e}")
            return None

    # Set up optimizer
    try:
        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)

        # Wrap optimizer if ZIO is enabled
        if apply_zio:
            optimizer = ZeroInfinityOptimizer(optimizer, model, zio_config)
            logger.info("Applied Zero-Infinity Offloading wrapper to optimizer")
    except Exception as e:
        logger.error(f"Error setting up optimizer: {e}")
        return None

    # Record memory after optimization setup
    memory_tracking.append(("optimizer_created", measure_memory_usage()))
    print_memory_stats("optimizer created", memory_tracking[-1][1])

    # Create dummy data
    try:
        dummy_input_ids = torch.randint(0, vocab_size, (batch_size, seq_length), device=device)
        dummy_attention_mask = torch.ones_like(dummy_input_ids, device=device)
        dummy_labels = torch.randint(0, vocab_size, (batch_size, seq_length), device=device)
    except Exception as e:
        logger.error(f"Error creating dummy data: {e}")
        return None

    # Record memory after data creation
    memory_tracking.append(("data_created", measure_memory_usage()))
    print_memory_stats("data created", memory_tracking[-1][1])

    # Training loop
    model.train()
    for i in range(num_iterations):
        try:
            # Forward pass
            outputs = model(
                input_ids=dummy_input_ids,
                attention_mask=dummy_attention_mask,
                labels=dummy_labels,
            )
            loss = outputs.loss

            # Record memory after forward pass
            if i == 0:
                memory_tracking.append((f"forward_pass_{i}", measure_memory_usage()))
                print_memory_stats(f"forward pass {i}", memory_tracking[-1][1])

            # Backward pass
            loss.backward()

            # Record memory after backward pass
            if i == 0:
                memory_tracking.append((f"backward_pass_{i}", measure_memory_usage()))
                print_memory_stats(f"backward pass {i}", memory_tracking[-1][1])

            # Optimizer step
            optimizer.step()
            optimizer.zero_grad()

            # Record memory after optimization step
            if i == 0:
                memory_tracking.append((f"optimization_step_{i}", measure_memory_usage()))
                print_memory_stats(f"optimization step {i}", memory_tracking[-1][1])

            logger.info(f"Iteration {i}, Loss: {loss.item():.4f}")

            # Clear any unused memory
            if i == 0 and apply_zio:
                torch.cuda.empty_cache()
                gc.collect()
        except Exception as e:
            logger.error(f"Error in training iteration {i}: {e}")
            break

    # Final memory usage
    memory_tracking.append(("final", measure_memory_usage()))
    print_memory_stats("final", memory_tracking[-1][1])

    # Return memory tracking data
    return memory_tracking


def print_comparison_results(results: Dict[str, Dict[str, List[Tuple[str, Dict[str, float]]]]]):
    """Print memory usage comparison results."""
    print("\n" + "="*80)
    print("Memory Usage Comparison: Standard vs. Zero-Infinity Offloading")
    print("="*80)

    for model_name, model_results in results.items():
        # Ensure we have both sets of results before comparing
        if "without_zio" not in model_results or "with_zio" not in model_results:
            print(f"\nWarning: Incomplete results for {model_name}")
            continue

        print(f"\nModel: {model_name}")
        print("-" * 60)

        try:
            # Get peak memory usage for each approach
            without_zio_peak_gpu = max(stage[1]["gpu_allocated_gb"] for stage in model_results["without_zio"])
            with_zio_peak_gpu = max(stage[1]["gpu_allocated_gb"] for stage in model_results["with_zio"])

            without_zio_peak_cpu = max(stage[1]["cpu_used_gb"] for stage in model_results["without_zio"])
            with_zio_peak_cpu = max(stage[1]["cpu_used_gb"] for stage in model_results["with_zio"])

            # Calculate memory reduction percentage
            if without_zio_peak_gpu > 0:
                gpu_reduction = (without_zio_peak_gpu - with_zio_peak_gpu) / without_zio_peak_gpu * 100
            else:
                gpu_reduction = 0

            print(f"Peak GPU Memory Usage:")
            print(f"  Standard:          {without_zio_peak_gpu:.2f} GB")
            print(f"  Zero-Infinity:     {with_zio_peak_gpu:.2f} GB")
            print(f"  Memory Reduction:  {gpu_reduction:.1f}%")

            print(f"\nPeak CPU Memory Usage:")
            print(f"  Standard:          {without_zio_peak_cpu:.2f} GB")
            print(f"  Zero-Infinity:     {with_zio_peak_cpu:.2f} GB")

            print("\nMemory Usage Breakdown by Stage:")
            stages_to_compare = ["model_loaded", "optimizer_created", "forward_pass_0", "backward_pass_0"]

            for stage in stages_to_compare:
                # Find the stage in both results
                without_stage = next((s for s in model_results["without_zio"] if s[0] == stage), None)
                with_stage = next((s for s in model_results["with_zio"] if s[0] == stage), None)

                if without_stage and with_stage:
                    print(f"\n  Stage: {stage}")
                    gpu_without = without_stage[1]["gpu_allocated_gb"]
                    gpu_with = with_stage[1]["gpu_allocated_gb"]

                    if gpu_without > 0:
                        reduction = (gpu_without - gpu_with) / gpu_without * 100
                    else:
                        reduction = 0

                    print(f"    Standard:         {gpu_without:.2f} GB")
                    print(f"    Zero-Infinity:    {gpu_with:.2f} GB")
                    print(f"    Memory Reduction: {reduction:.1f}%")

        except Exception as e:
            print(f"Error processing results for {model_name}: {e}")


def run_gpt2_comparison():
    """Run comparison with consistent batch sizes for accurate comparison."""
    results = {}
    model_name = "gpt2"

    print(f"\n{'='*40}\nTesting model: {model_name}\n{'='*40}")

    # Check GPU availability
    if not torch.cuda.is_available():
        print("No GPU available. This test requires a GPU to be meaningful.")
        return

    try:
        # Use consistent batch sizes for fair comparison
        batch_size = 4
        seq_length = 512

        # Without ZIO
        print(f"\nTraining WITHOUT Zero-Infinity Offloading")
        memory_without_zio = run_zio_training(
            model_name=model_name,
            apply_zio=False,
            batch_size=batch_size,
            seq_length=seq_length,
            num_iterations=2,
        )

        # Clear memory
        torch.cuda.empty_cache()
        gc.collect()
        time.sleep(2)  # Allow memory to fully clear

        # With ZIO
        print(f"\nTraining WITH Zero-Infinity Offloading")
        memory_with_zio = run_zio_training(
            model_name=model_name,
            apply_zio=True,
            batch_size=batch_size,
            seq_length=seq_length,  # Same sequence length for fair comparison
            num_iterations=2,
        )

        # Store results
        results[model_name] = {
            "without_zio": memory_without_zio,
            "with_zio": memory_with_zio,
        }

        # Print comparison results
        print_comparison_results(results)

    except Exception as e:
        print(f"Error running comparison: {e}")

if __name__ == "__main__":
    # Focus only on GPT-2 model
    print("Running GPT-2 Zero-Infinity Offloading Comparison")
    run_gpt2_comparison()